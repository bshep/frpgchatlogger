# Deployment Strategy for `frpgchatlogger` on AWS

This document outlines a strategy for deploying the `frpgchatlogger` application to an AWS server, ensuring it is accessible via a single URL and port. Given the project's Python backend and static frontend, a single Amazon EC2 instance with Nginx acting as a reverse proxy is a suitable initial approach.

## 1. Architecture Overview

The application will be deployed on a single Amazon EC2 instance.

*   **Amazon EC2 Instance**: A virtual server in AWS that will host all components of the application.
*   **Nginx**: Will act as a reverse proxy, listening on a standard HTTP/HTTPS port (e.g., 80 or 443). It will serve the static frontend files and proxy API requests to the backend application.
*   **Python Backend**: The `backend/main.py` application will run using a production-ready WSGI server like Gunicorn (or ASGI server like Uvicorn for FastAPI) behind Nginx.
*   **Frontend**: The static HTML, CSS, and JavaScript files generated by the frontend build process will be served directly by Nginx.
*   **Database**: The SQLite database will reside on the EC2 instance's file system, co-located with the backend application.

```
+-------------------+
|     AWS Cloud     |
|                   |
|  +------------------------------------------------+
|  |                EC2 Instance                    |
|  |                                                |
|  |    +------------------------+                  |
|  |    |         Nginx          |                  |
|  |    | (Reverse Proxy & Static|                  |
|  |    |     File Server)       |                  |
|  |    +-----------^------------+                  |
|  |                | (HTTP/S)                      |
|  |  +-------------+-----------+----------------+  |
|  |  | API Requests | Static Files (Frontend)   |  |
|  |  |              |                             |  |
|  |  +------+-------+------+--------------------+  |
|  |         | (HTTP - e.g., 8000)                 |  |
|  |         |                                     |  |
|  |  +------------------------+                  |  |
|  |  |    Python Backend      |                  |  |
|  |  | (Gunicorn/Uvicorn)     |                  |  |
|  |  +-----------^------------+                  |  |
|  |              |                                 |  |
|  |  +-----------+-----------+                  |  |
|  |  |    SQLite Database    |                  |  |
|  |  +------------------------+                  |  |
|  +------------------------------------------------+
|          ^                                        |
|          | (Public Internet - Single URL/PORT)    |
+----------|----------------------------------------+
           |
    User's Web Browser
```

## 2. AWS Services to Utilize

*   **Amazon EC2**: To provision a virtual machine instance.
*   **Amazon VPC**: For network isolation, subnets, and internet gateway.
*   **Security Groups**: To control inbound and outbound traffic to the EC2 instance, ensuring only necessary ports (e.g., 80, 443) are open to the public.
*   **Elastic IP (Optional but Recommended)**: To provide a static public IP address for the EC2 instance.
*   **Route 53 (Optional)**: If a custom domain name is desired, Route 53 can be used to map the domain to the Elastic IP.

## 3. Deployment Steps

The following steps outline the process to deploy the application:

### 3.1. EC2 Instance Provisioning

1.  **Launch EC2 Instance**: Choose an appropriate AMI (e.g., Amazon Linux 2, Ubuntu Server) and instance type (e.g., `t2.micro` for cost-effectiveness).
2.  **Configure Security Group**: Create a new security group allowing inbound HTTP (port 80) and HTTPS (port 443) traffic from anywhere (`0.0.0.0/0`, `::/0`). Also, allow SSH (port 22) from your IP address for management.
3.  **Key Pair**: Create or use an existing key pair for SSH access.
4.  **Elastic IP (Optional)**: Allocate and associate an Elastic IP address with the EC2 instance for a static public IP.

### 3.2. Instance Setup and Dependencies

1.  **SSH into EC2 Instance**: Connect using your key pair.
2.  **Update System**:
    ```bash
    sudo yum update -y # For Amazon Linux
    # Or for Ubuntu: sudo apt update && sudo apt upgrade -y
    ```
3.  **Install Python and pip**: Ensure Python 3 and pip are installed.
4.  **Install Nginx**:
    ```bash
    sudo yum install nginx -y # For Amazon Linux
    # Or for Ubuntu: sudo apt install nginx -y
    ```
5.  **Install Gunicorn/Uvicorn**:
    ```bash
    pip install gunicorn # Or uvicorn for FastAPI
    ```
6.  **Create Project Directory**: Create a directory for your application (e.g., `/var/www/frpgchatlogger`).

### 3.3. Application Deployment

1.  **Transfer Code**: Copy your `backend/` and `frontend/` directories to the EC2 instance (e.g., using `scp` or `git clone`).
    ```bash
    # Example using scp (replace with your actual paths and IP)
    scp -i /path/to/your/key.pem -r backend frontend ec2-user@YOUR_EC2_PUBLIC_IP:/var/www/frpgchatlogger/
    ```
2.  **Backend Setup**:
    *   Navigate to the backend directory: `cd /var/www/frpgchatlogger/backend`
    *   Create a Python virtual environment: `python3 -m venv venv`
    *   Activate the virtual environment: `source venv/bin/activate`
    *   Install backend dependencies: `pip install -r requirements.txt`
    *   Deactivate the virtual environment: `deactivate`
3.  **Frontend Build**:
    *   Navigate to the frontend directory: `cd /var/www/frpgchatlogger/frontend`
    *   Install frontend dependencies (assuming Node.js/npm is installed on EC2, or build locally and transfer): `npm install`
    *   Build the production-ready static assets: `npm run build` (This will typically generate a `dist/` or `build/` directory).
    *   Copy the built frontend assets to a location Nginx can serve, e.g., `/var/www/frpgchatlogger/html/`: `mkdir -p /var/www/frpgchatlogger/html && cp -r dist/* /var/www/frpgchatlogger/html/` (adjust `dist` to your actual build output directory).

### 3.4. Nginx Configuration

1.  **Configure Nginx**: Create a new Nginx configuration file (e.g., `/etc/nginx/conf.d/frpgchatlogger.conf`) or modify the default one.

    ```nginx
    server {
        listen 80;
        server_name YOUR_EC2_PUBLIC_IP_OR_DOMAIN; # Replace with your Elastic IP or domain

        location /api/ {
            proxy_pass http://127.0.0.1:8000; # Gunicorn/Uvicorn will listen on this port
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location / {
            root /var/www/frpgchatlogger/html; # Path to your built frontend files
            index index.html index.htm;
            try_files $uri $uri/ /index.html; # For single-page applications
        }

        error_log /var/log/nginx/frpgchatlogger_error.log;
        access_log /var/log/nginx/frpgchatlogger_access.log;
    }
    ```
    *Note: Replace `YOUR_EC2_PUBLIC_IP_OR_DOMAIN` with your EC2 instance's public IP address or the domain name you configure in Route 53.*

2.  **Test Nginx Configuration**: `sudo nginx -t`
3.  **Restart Nginx**: `sudo systemctl restart nginx`

### 3.5. Run Backend with Gunicorn/Uvicorn

1.  **Start Backend**: Navigate to the backend directory and activate the virtual environment. Then, run Gunicorn (or Uvicorn):
    ```bash
    cd /var/www/frpgchatlogger/backend
    source venv/bin/activate
    gunicorn -w 4 -b 127.0.0.1:8000 main:app # Replace 'main:app' with your actual FastAPI/Flask app entry point
    # Or for FastAPI: uvicorn main:app --host 127.0.0.1 --port 8000
    ```
    *Note: You would typically run this as a persistent service using `systemd` or `supervisor` in a production environment.*

### 3.6. Database Persistence

Since SQLite stores data in a file, its persistence on an EC2 instance depends on the instance's storage.

*   **Default Behavior**: Data stored on the root EBS volume will persist if the instance is stopped and started, but will be lost if the instance is terminated.
*   **Improved Persistence**: For better reliability and backup capabilities, consider:
    *   **Attaching a separate EBS volume**: Store the SQLite database file on a dedicated EBS volume that can be snapshotted and detached/re-attached.
    *   **Regular Backups**: Implement a strategy to regularly back up the SQLite database file to Amazon S3.
    *   **Migrating to a Managed Database (Future Enhancement)**: For higher availability, scalability, and managed backups, consider migrating the database to Amazon RDS (e.g., PostgreSQL or MySQL) or DynamoDB. This would require code changes to adapt the backend to a different database driver.

## 4. Single URL and Port Accessibility

By configuring Nginx to listen on port 80 (and potentially 443 with SSL/TLS), the entire application (both frontend and backend API) will be accessible through a single public IP address or domain name. Nginx handles the internal routing:

*   Requests to the base URL (e.g., `http://YOUR_EC2_PUBLIC_IP_OR_DOMAIN/`) will serve the static frontend files.
*   Requests to `/api/*` (e.g., `http://YOUR_EC2_PUBLIC_IP_OR_DOMAIN/api/messages`) will be proxied to the backend application running on `127.0.0.1:8000`.

This setup provides a unified access point for the entire application.

## 5. GitHub Actions for Automatic Deployment

To streamline the deployment process, GitHub Actions can be used to automatically build and deploy new code to the EC2 instance whenever changes are pushed to a specified branch (e.g., `main`).

### 5.1. Prerequisites for GitHub Actions

Before setting up the GitHub Action, you'll need the following:

1.  **SSH Key for EC2**: The private SSH key associated with the `.pem` file you use to connect to your EC2 instance. This key needs to be securely stored in GitHub Secrets.
2.  **EC2 Host Information**: The public IP address or hostname of your EC2 instance and the username used for SSH access (e.g., `ec2-user`, `ubuntu`).

### 5.2. GitHub Secrets Configuration

Store the sensitive information as GitHub Secrets in your repository. Go to `Settings > Secrets and variables > Actions > New repository secret`.

*   `EC2_SSH_PRIVATE_KEY`: Paste the *entire content* of your private SSH key file (including `-----BEGIN ... PRIVATE KEY-----` and `-----END ... PRIVATE KEY-----`).
*   `EC2_HOST`: The public IP address or hostname of your EC2 instance.
*   `EC2_USER`: The SSH username for your EC2 instance (e.g., `ec2-user`, `ubuntu`).

### 5.3. GitHub Actions Workflow

Create a new workflow file in your repository at `.github/workflows/aws_ec2_deploy.yml` with content similar to the following:

```yaml
name: Deploy to AWS EC2

on:
  push:
    branches:
      - main # Trigger this workflow on pushes to the 'main' branch

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18' # Use a Node.js version compatible with your frontend

    - name: Install Frontend Dependencies
      run: |
        cd frontend
        npm install

    - name: Build Frontend
      run: |
        cd frontend
        npm run build

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9' # Use a Python version compatible with your backend

    - name: Install Backend Dependencies
      run: |
        python -m venv venv_backend
        source venv_backend/bin/activate
        pip install -r backend/requirements.txt
        deactivate

    - name: Deploy to EC2
      env:
        SSH_PRIVATE_KEY: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
        EC2_HOST: ${{ secrets.EC2_HOST }}
        EC2_USER: ${{ secrets.EC2_USER }}
      run: |
        # Ensure SSH key has correct permissions
        echo "$SSH_PRIVATE_KEY" > deploy_key.pem
        chmod 600 deploy_key.pem

        # Define target directory on EC2
        DEPLOY_DIR="/var/www/frpgchatlogger"

        # Transfer backend files
        scp -o StrictHostKeyChecking=no -i deploy_key.pem -r backend $EC2_USER@$EC2_HOST:$DEPLOY_DIR/

        # Transfer frontend build output
        scp -o StrictHostKeyChecking=no -i deploy_key.pem -r frontend/dist/* $EC2_USER@$EC2_HOST:$DEPLOY_DIR/html/

        # SSH into EC2 and run deployment commands
        ssh -o StrictHostKeyChecking=no -i deploy_key.pem $EC2_USER@$EC2_HOST << 'EOF'
          # Navigate to project root
          cd /var/www/frpgchatlogger

          # Update backend dependencies (if requirements.txt changed)
          source venv_backend/bin/activate # Assuming venv_backend exists from initial setup
          pip install -r backend/requirements.txt
          deactivate

          # Restart Gunicorn/Uvicorn (assuming it's managed by systemd or similar)
          sudo systemctl restart frpgchatlogger_backend.service # Replace with your actual service name

          # Restart Nginx
          sudo systemctl restart nginx
EOF
```

### 5.4. Important Considerations for the Workflow

*   **Initial Setup**: The GitHub Actions workflow assumes that the EC2 instance has been initially set up (e.g., Nginx installed, Python virtual environment created, initial `npm install` and `npm run build` performed) as described in Section 3. The `venv_backend` is assumed to exist on the EC2 instance.
*   **Systemd Service**: For the backend (`gunicorn` or `uvicorn`) to be robustly managed, it should be configured as a `systemd` service on the EC2 instance. This allows it to restart automatically on server reboot or failure, and be easily managed with `sudo systemctl restart frpgchatlogger_backend.service`.
*   **Frontend `dist` directory**: Ensure the `frontend/dist/*` path matches the output directory of your `npm run build` command.
*   **StrictHostKeyChecking**: `StrictHostKeyChecking=no` is used for automation, but for production environments, consider pre-adding the EC2 host's public key to GitHub Actions' known hosts for enhanced security.
*   **Error Handling**: This example is basic. For production, add more robust error handling, notifications, and potentially rollback mechanisms.

## 6. Automated EC2 Instance Provisioning

While Section 3 describes manual steps for setting up an EC2 instance, automating this provisioning process is crucial for ensuring consistency, reproducibility, and disaster recovery. Infrastructure as Code (IaC) practices allow you to define your infrastructure in code, which can then be version-controlled and deployed automatically.

### 6.1. Why Automate Provisioning?

*   **Consistency**: Ensures every EC2 instance is set up identically, reducing configuration drift and errors.
*   **Reproducibility**: Easily recreate your environment (e.g., for testing, scaling, or disaster recovery).
*   **Speed**: Provision new instances quickly and efficiently.
*   **Version Control**: Track changes to your infrastructure definitions over time.

### 6.2. Tools for Automated Provisioning

Several tools can help automate EC2 provisioning:

*   **AWS User Data Scripts**: This is the simplest method for basic automation. When launching an EC2 instance, you can provide a script (bash or PowerShell) that runs once during the instance's first boot. This script can install software, update packages, configure services, and perform other initial setup tasks.
*   **AWS CloudFormation**: AWS's native IaC service. You define your entire AWS infrastructure (EC2 instances, security groups, VPCs, databases, etc.) in a JSON or YAML template. CloudFormation then provisions and updates these resources in a controlled and repeatable manner.
*   **Terraform**: A popular open-source IaC tool that supports multiple cloud providers, including AWS. It allows you to define your infrastructure using HashiCorp Configuration Language (HCL) and manage resources across different environments.
*   **Configuration Management Tools (Ansible, Chef, Puppet)**: These tools are primarily used for managing software configuration on existing servers. While they can be used for initial setup, they are often combined with IaC tools (like CloudFormation or User Data) where IaC provisions the server, and then the configuration management tool configures the software on that server.

### 6.3. Automating Initial Setup with User Data

For the initial setup described in Section 3.2 (Instance Setup and Dependencies) and parts of Section 3.4 (Nginx Configuration) and 3.5 (Run Backend with Gunicorn/Uvicorn), an AWS User Data script can significantly reduce manual effort. This script will run as `root` on the first boot of your EC2 instance.

Below is an example of a User Data script for an Amazon Linux 2 instance. This script should be provided when you launch the EC2 instance.

```bash
#!/bin/bash
exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1
echo "Starting User Data script..."

# 1. Update System
sudo yum update -y

# 2. Install Python, pip, Node.js, and Nginx
# For Amazon Linux, Python 3 is often python3.x, pip is pip3
sudo yum install -y python3-pip

# Install Node.js (example for Amazon Linux 2)
# Using nvm or a package manager
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
. ~/.nvm/nvm.sh
nvm install node # Installs the latest LTS version of Node.js

sudo yum install -y nginx

# 3. Install Gunicorn/Uvicorn globally or for a specific user (preferred in venv later)
# For simplicity in user-data, we'll ensure it's available.
# In a real setup, it would be in the app's venv.
pip3 install gunicorn

# 4. Create Project Directory (if not already created by deployment action)
# The GitHub Action will transfer code to /var/www/frpgchatlogger
# We'll create the base HTML directory here
mkdir -p /var/www/frpgchatlogger/html

# 5. Configure Nginx (Example - this would need to be more dynamic or templated for production)
# Create the Nginx config file. Note: server_name should be dynamic or specific.
cat << 'EOF' | sudo tee /etc/nginx/conf.d/frpgchatlogger.conf
server {
    listen 80;
    server_name _; # Listen on all hostnames or specify YOUR_EC2_PUBLIC_IP_OR_DOMAIN;

    location /api/ {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    location / {
        root /var/www/frpgchatlogger/html;
        index index.html index.htm;
        try_files $uri $uri/ /index.html;
    }

    error_log /var/log/nginx/frpgchatlogger_error.log;
    access_log /var/log/nginx/frpgchatlogger_access.log;
}
EOF

# Ensure Nginx starts on boot
sudo systemctl enable nginx
sudo systemctl start nginx

# 6. Create Systemd Service for Backend (Placeholder - requires actual app code)
# This assumes your Gunicorn/Uvicorn setup is in /var/www/frpgchatlogger/backend
# and uses the venv 'venv_backend' as mentioned in GitHub Actions.
# The actual script for starting the app would be more robust.
cat << 'EOF' | sudo tee /etc/systemd/system/frpgchatlogger_backend.service
[Unit]
Description=Gunicorn/Uvicorn instance for frpgchatlogger
After=network.target

[Service]
User=ec2-user # Or appropriate user
Group=nginx # Or appropriate group for Nginx to access
WorkingDirectory=/var/www/frpgchatlogger/backend
ExecStart=/var/www/frpgchatlogger/backend/venv_backend/bin/gunicorn --workers 4 --bind unix:/tmp/frpgchatlogger.sock main:app # Adjust as needed
Restart=always
PrivateTmp=true

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl enable frpgchatlogger_backend.service
# Note: The backend service will fail to start until code is deployed and venv_backend is created.
# This setup assumes the GitHub Action will deploy the code and trigger a restart.

echo "User Data script finished."
```

### 6.4. Integrating with CloudFormation/Terraform

For a fully automated IaC approach, you would embed the User Data script (or a more sophisticated setup using configuration management tools) within your CloudFormation or Terraform templates. These templates would define:

*   **EC2 Instance**: AMI, instance type, key pair, VPC, subnets, security groups, and the User Data script.
*   **Elastic IP**: Association with the EC2 instance.
*   **Security Groups**: Ingress/egress rules.
*   **IAM Roles/Policies**: If the EC2 instance needs to interact with other AWS services.

This allows you to launch a completely pre-configured and ready-to-receive-code EC2 instance with a single command.